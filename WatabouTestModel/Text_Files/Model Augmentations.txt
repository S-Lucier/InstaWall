DATA AUGMENTATION GUIDE FOR DUNGEON MAP SEGMENTATION
=====================================================

Location: dataset.py → get_train_transforms()

All augmentation parameters are grouped in one code block for easy tuning.
These augmentations are applied ON-THE-FLY during training to both image and mask.


CATEGORY 1: PREPROCESSING
==========================

A.LongestMaxSize(max_size=512)
  Purpose: Scale image so longest dimension equals 512px
  Preserves: Aspect ratio
  Affects: Both image and mask
  Tuning: Match your training resolution (512, 768, 1024, etc.)

A.PadIfNeeded(min_height=512, min_width=512, border_mode=0)
  Purpose: Add black padding to make image exactly 512×512
  Preserves: Aspect ratio (padding is added symmetrically)
  Affects: Both image and mask
  Tuning: Match your training resolution
  Note: border_mode=0 means pad with black (value=0)


CATEGORY 2: GEOMETRIC AUGMENTATIONS
====================================

A.RandomRotate90(p=1.0)
  Purpose: Rotate by 0°, 90°, 180°, or 270° (grid-aligned only)
  Why: TTRPG maps are direction-agnostic; north can be any direction
  Benefit: 4x data multiplier, teaches rotational invariance
  Affects: Both image and mask (keeps them aligned)
  Parameter: p=1.0 means always apply (always rotate by some multiple of 90°)
  Tuning Range: p=0.5 to 1.0 (lower = sometimes no rotation)

  CRITICAL: Do NOT use A.Rotate(limit=X) for arbitrary angles!
            TTRPG maps are grid-aligned; non-90° rotations create
            unrealistic training data and hurt performance.

A.HorizontalFlip(p=0.5)
  Purpose: Flip image left-right
  Why: Maps work when mirrored; no inherent left/right orientation
  Benefit: 2x data multiplier
  Affects: Both image and mask
  Parameter: p=0.5 means 50% chance
  Tuning Range: p=0.3 to 0.7
  Note: Combined with rotations and vertical flip = 8x data

A.VerticalFlip(p=0.5)
  Purpose: Flip image top-bottom
  Why: Maps work when mirrored vertically
  Benefit: 2x data multiplier (with horizontal flip)
  Affects: Both image and mask
  Parameter: p=0.5 means 50% chance
  Tuning Range: p=0.3 to 0.7


CATEGORY 3: COLOR/BRIGHTNESS AUGMENTATIONS
===========================================

A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5)
  Purpose: Adjust overall brightness and contrast
  Why: Simulates different lighting, scan quality, printer darkness
  Examples:
    - Brightness +20%: Map looks lighter (faded scan)
    - Brightness -20%: Map looks darker (dark scan/photo)
    - Contrast +20%: Walls are darker, floors are lighter (high contrast)
    - Contrast -20%: Everything looks more gray (washed out scan)
  Affects: Image only (masks unchanged)
  Parameters:
    - brightness_limit=0.2 means ±20% brightness adjustment
    - contrast_limit=0.2 means ±20% contrast adjustment
    - p=0.5 means apply to 50% of images
  Tuning Range:
    - brightness_limit: 0.1 (subtle) to 0.3 (strong)
    - contrast_limit: 0.1 (subtle) to 0.3 (strong)
    - p: 0.3 to 0.7

A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.3)
  Purpose: Subtle color variations and shifts
  Why: Different paper colors, aging, color printer vs B&W, lighting tint
  Examples:
    - Saturation: Affects how "colorful" the image is
    - Hue: Shifts colors (yellow tint from aging, blue tint from lighting)
  Affects: Image only (masks unchanged)
  Parameters:
    - brightness=0.1 means ±10% brightness jitter
    - contrast=0.1 means ±10% contrast jitter
    - saturation=0.1 means ±10% saturation shift
    - hue=0.05 means ±5% hue shift (subtle color tinting)
    - p=0.3 means apply to 30% of images
  Tuning Range:
    - brightness/contrast/saturation: 0.05 to 0.2
    - hue: 0.02 to 0.1 (keep subtle)
    - p: 0.2 to 0.5
  Note: More subtle than RandomBrightnessContrast; use both for variety


CATEGORY 4: QUALITY DEGRADATION
================================

A.GaussNoise(var_limit=(5.0, 20.0), p=0.2)
  Purpose: Add random pixel noise
  Why: Simulates scan artifacts, JPEG compression, photo grain
  Examples:
    - Low variance (5): Very subtle grain
    - High variance (20): Visible noise but not overwhelming
  Affects: Image only (masks unchanged)
  Parameters:
    - var_limit=(5.0, 20.0) means variance between 5 and 20
    - p=0.2 means apply to 20% of images
  Tuning Range:
    - var_limit: (3.0, 15.0) for subtle, (10.0, 30.0) for stronger
    - p: 0.1 to 0.3 (don't overuse; too much noise hurts learning)
  Note: Variance is squared standard deviation; 20 = ~4.5 pixel std dev

A.Blur(blur_limit=3, p=0.2)
  Purpose: Slight blur/defocus
  Why: Simulates out-of-focus photos, scan blur, motion blur
  Examples:
    - blur_limit=3 means kernel size of 1×1 or 3×3 (very subtle)
    - Makes edges slightly softer
  Affects: Image only (masks unchanged)
  Parameters:
    - blur_limit=3 means maximum kernel size of 3 pixels
    - p=0.2 means apply to 20% of images
  Tuning Range:
    - blur_limit: 3 (subtle) to 7 (moderate) - don't go higher
    - p: 0.1 to 0.3
  Note: Too much blur makes walls hard to detect; keep subtle


CATEGORY 5: NORMALIZATION
==========================

A.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0], max_pixel_value=255.0)
  Purpose: Scale pixel values from [0-255] to [0.0-1.0]
  Why: Neural networks work better with small numbers (0-1 range)
  Math: normalized = (pixel / 255.0 - mean) / std
  With these parameters: normalized = pixel / 255.0
  Examples:
    - Black pixel (0) → 0.0
    - Gray pixel (128) → 0.5
    - White pixel (255) → 1.0
  Affects: Image only (masks are categorical labels, stay 0/127/255)
  Parameters:
    - mean=[0.0, 0.0, 0.0] means no mean centering
    - std=[1.0, 1.0, 1.0] means no std deviation scaling
    - max_pixel_value=255.0 is the original range
  Tuning: Generally don't change unless using pre-trained weights
  Note: If using ImageNet pre-trained weights, use:
        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]

ToTensorV2()
  Purpose: Convert numpy array to PyTorch tensor
  Format: Changes from (H, W, C) to (C, H, W) for PyTorch convention
  Required: Must be last transform in pipeline


EFFECTIVE DATA MULTIPLIER
==========================

With current augmentations (54 training images):
  - RandomRotate90: 4 possible rotations
  - HorizontalFlip: 2 states (flipped or not)
  - VerticalFlip: 2 states (flipped or not)

Geometric only: 4 × 2 × 2 = 16x base multiplier
With color/quality variations: Effectively 100+ variations per image

Total effective training set: ~5,400+ unique variations from 54 images


VALIDATION TRANSFORMS
=====================

Validation uses NO augmentation (only preprocessing and normalization):
  - LongestMaxSize + PadIfNeeded (same as training)
  - Normalize (same as training)
  - ToTensorV2

Why: We want consistent validation metrics, not randomized results


TUNING GUIDELINES
=================

Start Conservative:
  - Use default parameters first
  - Train for 20-50 epochs, evaluate
  - If overfitting (train acc >> val acc): increase augmentation
  - If underfitting (both train/val acc low): decrease augmentation

Adjust by Category:
  1. Geometric: Usually keep at default (p=0.5 to 1.0)
  2. Color/Brightness: Tune based on your data variety
     - More varied data → lower augmentation needed
     - Uniform data (all Watabou) → higher augmentation helps
  3. Quality Degradation: Tune based on inference data
     - Digital maps only → low or disable (p=0.1)
     - Photos/scans → higher (p=0.3 to 0.5)

Signs of Over-Augmentation:
  - Training loss stops decreasing
  - Model takes much longer to converge
  - Validation accuracy doesn't improve
  → Reduce probability (p values) or intensity (limit values)

Signs of Under-Augmentation:
  - High training accuracy (>99%) but low validation accuracy (<95%)
  - Model fails on slightly different looking maps
  - Overfitting is clear
  → Increase probability or add more augmentation types


WHAT NOT TO DO
==============

❌ A.Rotate(limit=35) - Arbitrary angle rotation
   Why: Creates unrealistic non-grid-aligned training data

❌ A.RandomCrop() - Random cropping
   Why: Might cut off important dungeon areas, breaks spatial context

❌ A.ElasticTransform() - Warping/distortion
   Why: TTRPG maps have rigid geometry; warping breaks walls/doors

❌ Excessive blur (blur_limit>7) or noise (var_limit>50)
   Why: Makes walls genuinely hard to detect, not just challenging

❌ Extreme color shifts (hue>0.2)
   Why: Can invert walls/floors if colors shift too far


DEBUGGING AUGMENTATIONS
========================

To visualize what augmentations are doing:

```python
from dataset import SegmentationDataset, get_train_transforms
import matplotlib.pyplot as plt

dataset = SegmentationDataset(
    "data/train_images",
    "data/train_masks",
    transform=get_train_transforms()
)

# Load same image multiple times to see variations
img, mask = dataset[0]
img2, mask2 = dataset[0]
img3, mask3 = dataset[0]

# Plot to see different augmentations
fig, axes = plt.subplots(3, 2, figsize=(10, 15))
axes[0][0].imshow(img.permute(1, 2, 0))
axes[0][1].imshow(mask, cmap='gray')
# ... repeat for img2, img3
plt.show()
```


PARAMETER SUMMARY TABLE
=======================

NOTE: Current settings use "Aggressive" values - optimized for small dataset (54 images)

Transform                   | Parameter           | Default | Conservative | Aggressive (CURRENT)
----------------------------|---------------------|---------|--------------|---------------------
RandomRotate90              | p                   | 1.0     | 0.7          | 1.0
HorizontalFlip              | p                   | 0.7     | 0.3          | 0.7
VerticalFlip                | p                   | 0.7     | 0.3          | 0.7
RandomBrightnessContrast    | brightness_limit    | 0.25    | 0.1          | 0.3
RandomBrightnessContrast    | contrast_limit      | 0.25    | 0.1          | 0.3
RandomBrightnessContrast    | p                   | 0.6     | 0.3          | 0.7
ColorJitter                 | brightness          | 0.15    | 0.05         | 0.2
ColorJitter                 | contrast            | 0.15    | 0.05         | 0.2
ColorJitter                 | saturation          | 0.15    | 0.05         | 0.2
ColorJitter                 | hue                 | 0.08    | 0.02         | 0.1
ColorJitter                 | p                   | 0.4     | 0.2          | 0.5
GaussNoise                  | var_limit           | (5, 20) | (3, 15)      | (10, 30)
GaussNoise                  | p                   | 0.2     | 0.1          | 0.3
Blur                        | blur_limit          | 3       | 3            | 5
Blur                        | p                   | 0.2     | 0.1          | 0.3


NOTES
=====

- Augmentations are cumulative and probabilistic; each is applied independently
- Multiple augmentations can apply to the same image in one training iteration
- Geometric transforms (rotate, flip) always maintain image-mask alignment
- Pixel transforms (color, noise, blur) only affect images, not masks
- All augmentations happen in CPU/RAM before GPU training
- More augmentation = slower training (more CPU preprocessing per batch)
- The model never sees the "true" original images during training, only augmented versions
